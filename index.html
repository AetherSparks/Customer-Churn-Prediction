<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Customer Churn Prediction Model</title>
    <link rel="stylesheet" href="styles.css"> <!-- Corrected path to styles.css -->
</head>
<body>
    <header>
        <div class="container">
            <h1>Customer Churn Prediction Model</h1>
            <p class="subtitle">Using Logistic Regression, Random Forest Classifier, and Data Preprocessing Techniques for Accurate Predictions</p>
        </div>
    </header>
    
    <section id="project-info">
        <div class="container">
            <h2>Project Overview</h2>
            <p>The <strong>Customer Churn Prediction Model</strong> is a data science project developed to predict the likelihood of a customer canceling their subscription (churning) with a telecommunications company. By using machine learning techniques such as <strong>Logistic Regression</strong> and <strong>Random Forest Classifier</strong>, the model analyzes various customer features to predict churn. The project helps to identify customers at risk of leaving, enabling companies to implement retention strategies to minimize churn.</p>
            <p>This project relies heavily on the Telco Customer Churn dataset, which includes customer data such as their demographics, account information, and usage patterns. Through rigorous data preprocessing, feature engineering, and model tuning, the goal is to achieve high accuracy and precision in the churn predictions.</p>
            <p></p>
        </div>
    </section>

    <!-- Added Project Report Details Section -->
    <section id="report-details">
        <div class="container">
            <h2>Project Report</h2>
            <p><strong>Completed By:</strong><br>
            Abhiraj Ghose (E23CSEU0014)<br>
            Pallav Sharma (E23CSEU0022)<br>
            Vaibhav Gupta (E23CSEU0112)</p>
            <p>Under the guidance of Mr. Prashant Kapil </p>

            <p><strong>Submitted To:</strong><br>
            School Of Computer Science Engineering and Technology, Bennett University<br>
            Greater Noida, 201310, Uttar Pradesh, India<br>
            </p>
        </div>
    </section>

    <section id="project-outline">
        <div class="container">
            <h2>Project Outline</h2>
            
            <!-- Tools and Technologies Used -->
            <div class="outline-item">
                <h3>1. Tools and Technologies Used</h3>
                <p>In this project, a variety of tools and technologies were used to build, preprocess, and deploy the churn prediction model. These tools provide powerful features for machine learning workflows, ensuring the project achieves its desired outcomes:</p>
                <ul>
                    <li><strong>Python</strong>: The primary programming language used for data analysis, model development, and visualization. Python provides a rich ecosystem of libraries and frameworks for machine learning and data science tasks.</li>
                    <li><strong>Jupyter Notebook</strong>: Used for interactive code execution, making it easy to experiment with different algorithms and visualize results.</li>
                    <strong>pandas</strong> & <strong>NumPy</strong>: These libraries are essential for data manipulation and numerical computations. They were used to clean, preprocess, and analyze the dataset.</li>
                    <li><strong>Matplotlib</strong> & <strong>Seaborn</strong>: Libraries for creating visualizations, including charts, graphs, and heatmaps, which helped in understanding data patterns and evaluating the model.</li>
                    <li><strong>Scikit-learn</strong>: A powerful toolkit for machine learning that provides tools for model building, evaluation, and optimization.</li>
                    <li><strong>XGBoost</strong>: An advanced machine learning algorithm that optimizes model accuracy, particularly for structured/tabular data.</li>
                    <li><strong>SHAP</strong>: SHAP (Shapley Additive Explanations) was used for explaining model predictions, providing insights into the contribution of each feature to the final output.</li>
                </ul>
            </div>

            <!-- Dataset -->
            <div class="outline-item">
                <h3>2. Dataset Used</h3>
                <p>The <strong>Telco Customer Churn dataset</strong> is publicly available and contains information about the customers of a telecommunications company. The dataset has 7,043 customer records and 21 features, including both categorical and numerical variables. Some of the key features in the dataset include:</p>
                <ul>
                    <li><strong>gender</strong>: The gender of the customer (Male or Female).</li>
                    <li><strong>tenure</strong>: The number of months the customer has been subscribed to the company.</li>
                    <li><strong>monthlycharges</strong>: The monthly charge for the customerâ€™s service.</li>
                    <li><strong>totalcharges</strong>: The total amount spent by the customer over the course of their subscription.</li>
                    <li><strong>contract</strong>: The type of contract the customer has (Month-to-Month, One Year, or Two Year).</li>
                    <li><strong>paymentmethod</strong>: The method of payment used by the customer (e.g., Electronic Check, Mailed Check).</li>
                </ul>
                <p>The target variable is <strong>Churn</strong>, which indicates whether the customer has left the company (1) or stayed (0). The dataset is balanced with a reasonable mix of churn and non-churn customers.</p>
            </div>

            <!-- Model Training and Evaluation -->
            <div class="outline-item">
                <h3>3. Model Training and Evaluation</h3>
                <p>Once the data was preprocessed, the next step was training several machine learning models to predict customer churn. The following models were implemented and evaluated:</p>
                <ul>
                    <li><strong>Logistic Regression</strong>: A statistical model used as a baseline for comparison with more complex models.</li>
                    <li><strong>Random Forest Classifier</strong>: A powerful ensemble method that constructs a series of decision trees and aggregates their predictions for improved accuracy.</li>
                    <li><strong>XGBoost</strong>: A gradient boosting algorithm that is known for its high performance on tabular data.</li>
                </ul>
                <p>After training the models, they were evaluated using the following metrics:</p>
                <ul>
                    <li><strong>Accuracy</strong>: The percentage of correct predictions made by the model.</li>
                    <li><strong>Precision</strong>: The ratio of true positive predictions among all positive predictions made by the model.</li>
                    <li><strong>Recall</strong>: The proportion of true positive predictions among all actual positive instances.</li>
                    <li><strong>F1 Score</strong>: The harmonic mean of precision and recall, which balances both metrics.</li>
                    <li><strong>ROC-AUC Curve</strong>: A graphical representation of the trade-off between the true positive rate and false positive rate.</li>
                </ul>
                <p>The Random Forest Classifier achieved the highest performance, with a strong balance of precision, recall, and accuracy, making it the best model for this task.</p>
            </div>

            <!-- Data Preprocessing and Feature Engineering -->
            <div class="outline-item">
                <h3>4. Data Preprocessing and Feature Engineering</h3>
                <p>Data preprocessing is a critical step in machine learning workflows. In this project, the following preprocessing steps were undertaken:</p>
                <ul>
                    <li><strong>Missing Data Handling</strong>: Missing values were imputed using appropriate strategies, ensuring the dataset was complete for model training.</li>
                    <li><strong>Feature Scaling</strong>: Numerical features like <strong>monthlycharges</strong> and <strong>tenure</strong> were scaled to standardize their range, improving the performance of certain models like Logistic Regression.</li>
                    <li><strong>One-Hot Encoding</strong>: Categorical variables such as <strong>contract</strong>, <strong>paymentmethod</strong>, and <strong>gender</strong> were converted into dummy variables to make them suitable for machine learning algorithms.</li>
                    <li><strong>Feature Creation</strong>: New features like <strong>customer_age</strong> (calculated from tenure) were engineered to provide more context for the models.</li>
                </ul>
                <p>These preprocessing steps were crucial for ensuring that the models could learn the relationships between features effectively and make accurate predictions.</p>
            </div>
        </div>
    </section>

    <section id="visualizations">
        <div class="container">
            <h2>Visualizations</h2>
            <p>Below are some of the key visualizations created during the project to better understand the dataset and model performance:</p>

            <!-- Visualization 1: Correlation Heatmap -->
            <div class="image-gallery">
                <h3>1. Correlation Heatmap</h3>
                <img src="static/images/correlation_heatmap.png" alt="Correlation Heatmap">
                <p>This heatmap visualizes the correlation between different numerical features in the dataset, helping to identify potential relationships and multicollinearity.</p>
            </div>

            <!-- Visualization 2: Churn Count Plot -->
            <div class="image-gallery">
                <h3>2. Churn Count Plot</h3>
                <img src="static/images/churn_count_plot.png" alt="Churn Count Plot">
                <p>The churn count plot shows the distribution of customers who have churned versus those who have not. This gives an overview of the class imbalance in the dataset.</p>
            </div>

            <!-- Visualization 3: Gender vs Churn Pie Chart -->
            <div class="image-gallery">
                <h3>3. Gender vs Churn Pie Chart</h3>
                <img src="static/images/gender_churn_pie_charts.png" alt="Gender vs Churn Pie Chart">
                <p>This pie chart breaks down the churn rate by gender, highlighting any significant differences between male and female customers in terms of churn behavior.</p>
            </div>

            <!-- Visualization 4: Churn by Gender Pie Chart -->
            <div class="image-gallery">
                <h3>4. Churn by Gender Pie Chart</h3>
                <img src="static/images/churn_by_gender_pie_chart.png" alt="Churn by Gender Pie Chart">
                <p>This pie chart displays the proportion of churned customers by gender, helping to understand if there are any gender-based trends in customer attrition.</p>
            </div>

            <!-- Visualization 5: Confusion Matrix -->
            <div class="image-gallery">
                <h3>5. Confusion Matrix</h3>
                <img src="static/images/confusion_matrix.png" alt="Confusion Matrix">
                <p>The confusion matrix provides a visual representation of the modelâ€™s performance on binary classification tasks. It shows the number of true positives, true negatives, false positives, and false negatives.</p>
            </div>

            <!-- Visualization 6: Feature Importance (Random Forest) -->
            <div class="image-gallery">
                <h3>6. Feature Importance (Random Forest)</h3>
                <img src="static/images/feature_importance.png" alt="Feature Importance (Random Forest)">
                <p>This plot shows the importance of different features as determined by the Random Forest model. It helps to understand which features contribute most to the churn prediction, guiding future improvements in model development.</p>
            </div>

            <!-- Visualization 7: SHAP Summary Plot -->
            <div class="image-gallery">
                <h3>7. SHAP Summary Plot</h3>
                <img src="static/images/shap_summary_plot.png" alt="SHAP Summary Plot">
                <p>The SHAP summary plot visualizes the impact of each feature on model predictions, providing insights into the modelâ€™s decision-making process and ensuring transparency in predictions.</p>
            </div>

        </div>
    </section>
    <section id="links">
        <div class="container">
            <h2>Project Links</h2>
            <ul>
                <li><a href="https://github.com/AetherSparks/Customer-Churn-Prediction" target="_blank">GitHub Repository</a></li>
                <li>You can find the Website Link, Project Report, PPT, Source Code, Dataset, Trained Model, etc. in the Github Repository.</li>
            </ul>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>Made By Abhiraj Ghose, Pallav Sharma, Vaibhav Gupta, under the guidance of Mr. Prashant Kapil.</p>
        </div>
    </footer>
</body>
</html>
